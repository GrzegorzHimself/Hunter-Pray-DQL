{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCL5X9pAGqiPql3MjArszC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrzegorzHimself/Hunter-Prey-IQL/blob/main/HnP_Conv2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdKDWI3A10cc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as anime\n",
        "from matplotlib.animation import PillowWriter\n",
        "import torch.nn.functional as F\n",
        "import heapq\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# ------------------- REPLAY BUFFER ------------------- #\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, map_tensor, scalar_tensor, action, reward, next_map_tensor, next_scalar_tensor, done):\n",
        "        # Replay buffer setup for shuffle and mini-batch creation\n",
        "        self.buffer.append((map_tensor, scalar_tensor, action, reward, next_map_tensor, next_scalar_tensor, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        (map_tensors, scalar_tensors,\n",
        "         actions, rewards,\n",
        "         next_map_tensors, next_scalar_tensors,\n",
        "         dones) = zip(*batch)\n",
        "\n",
        "        # Convert them into proper torch tensors\n",
        "        map_tensors         = torch.stack(map_tensors).to(device)              # shape: [B, 1, grid, grid]\n",
        "        scalar_tensors      = torch.stack(scalar_tensors).to(device)           # shape: [B, 8]\n",
        "        next_map_tensors    = torch.stack(next_map_tensors).to(device)         # shape: [B, 1, grid, grid]\n",
        "        next_scalar_tensors = torch.stack(next_scalar_tensors).to(device)      # shape: [B, 8]\n",
        "\n",
        "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        dones   = torch.tensor(dones,   dtype=torch.float32).to(device)\n",
        "\n",
        "        return map_tensors, scalar_tensors, actions, rewards, next_map_tensors, next_scalar_tensors, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# ------------------- DQN NETWORK (Conv2d + MLP) ------------------- #\n",
        "class ConvDQN(nn.Module):\n",
        "    def __init__(self, grid_size, scalar_size, n_actions):\n",
        "        super(ConvDQN, self).__init__()\n",
        "\n",
        "        # We'll parse a single-channel local_map: shape [B, 1, grid_size, grid_size]\n",
        "        # + we have \"scalar_size\" scalar features (like [x_h, y_h, x_p, y_p, sees_h, sees_p, dx, dy])\n",
        "\n",
        "        # Convolution block (stride=1, padding=1, kernel=3 => shape stays the same)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8,  kernel_size=3, padding=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        # We'll do a \"dummy\" pass to figure out how many features come out of conv2\n",
        "        # so that we can safely build our linear layers\n",
        "        self.grid_size   = grid_size\n",
        "        self.scalar_size = scalar_size\n",
        "        self.n_actions   = n_actions\n",
        "\n",
        "        # We do a dummy forward to compute conv_out_dim:\n",
        "        with torch.no_grad():\n",
        "            dummy_map = torch.zeros(1, 1, grid_size, grid_size)  # [batch=1, channel=1, g, g]\n",
        "            x = self.conv1(dummy_map)\n",
        "            x = self.conv2(x)\n",
        "            # x.shape = [1, 16, grid_size, grid_size] if all is correct\n",
        "            self.conv_out_dim = x.numel()  # Total elements\n",
        "            # That should be 1 * 16 * grid_size * grid_size\n",
        "\n",
        "        # We'll combine conv output + scalar_size in a linear layer\n",
        "        self.fc1 = nn.Linear(self.conv_out_dim + scalar_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, map_tensor, scalar_tensor):\n",
        "        # map_tensor shape: [B, 1, grid_size, grid_size]\n",
        "        # scalar_tensor shape: [B, scalar_size]\n",
        "\n",
        "        x = F.relu(self.conv1(map_tensor))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # Flatten dynamically\n",
        "        x = x.view(x.size(0), -1)  # shape: [B, conv_out_dim]\n",
        "\n",
        "        # Concat with scalar input\n",
        "        x = torch.cat([x, scalar_tensor], dim=1)  # shape: [B, conv_out_dim + scalar_size]\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        q = self.fc3(x)\n",
        "        return q\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- AGENT ------------------- #\n",
        "class Agent:\n",
        "    def __init__(self, grid_size, scalar_size, n_actions,\n",
        "                 gamma=0.99, epsilon=1.0,\n",
        "                 epsilon_decay=0.995, epsilon_min=0.1, lr=0.001):\n",
        "        self.grid_size    = grid_size\n",
        "        self.scalar_size  = scalar_size\n",
        "        self.n_actions    = n_actions\n",
        "        self.gamma        = gamma\n",
        "        self.epsilon      = epsilon\n",
        "        self.epsilon_decay= epsilon_decay\n",
        "        self.epsilon_min  = epsilon_min\n",
        "\n",
        "        # Build the ConvDQN\n",
        "        self.model = ConvDQN(grid_size, scalar_size, n_actions).to(device)\n",
        "        self.target_model = ConvDQN(grid_size, scalar_size, n_actions).to(device)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        self.optimizer   = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion   = nn.MSELoss()\n",
        "        self.replay_buffer = ReplayBuffer(50000)\n",
        "\n",
        "    def predict(self, map_tensor, scalar_tensor):\n",
        "        # Epsilon-greedy\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                # Shape for map_tensor => [1, 1, g, g]\n",
        "                # Shape for scalar_tensor => [1, scalar_size]\n",
        "                map_tensor    = map_tensor.unsqueeze(0).to(device)\n",
        "                scalar_tensor = scalar_tensor.unsqueeze(0).to(device)\n",
        "                q_values      = self.model(map_tensor, scalar_tensor)  # => [1, n_actions]\n",
        "                action        = torch.argmax(q_values, dim=1).item()\n",
        "                return action\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "    def train(self, batch_size):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        (map_tensors, scalar_tensors,\n",
        "         actions, rewards,\n",
        "         next_map_tensors, next_scalar_tensors,\n",
        "         dones) = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        # map_tensors.shape => [B, 1, g, g]\n",
        "        # scalar_tensors.shape => [B, scalar_size]\n",
        "        # actions.shape => [B]\n",
        "        # rewards.shape => [B]\n",
        "        # next_map_tensors.shape => [B, 1, g, g]\n",
        "        # next_scalar_tensors.shape => [B, scalar_size]\n",
        "        # dones.shape => [B]\n",
        "\n",
        "        # Current Q-values\n",
        "        q_values = self.model(map_tensors, scalar_tensors)  # shape [B, n_actions]\n",
        "        q_values = q_values.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Next Q-values\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_model(next_map_tensors, next_scalar_tensors).max(1)[0]\n",
        "\n",
        "        # Target Q-values\n",
        "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        loss = self.criterion(q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "# ------------------- PLAYER ------------------- #\n",
        "class Player:\n",
        "    def __init__(self, x, y, fov_radius, grid_size):\n",
        "        self.position   = [x, y]\n",
        "        self.fov_radius = fov_radius\n",
        "        self.grid_size  = grid_size\n",
        "        self.vision     = []\n",
        "        # local_map: 2D array (grid_size x grid_size),\n",
        "        #   0=unknown, 1=free cell, 2=wall\n",
        "        self.local_map  = np.zeros((grid_size, grid_size), dtype=np.int8)\n",
        "\n",
        "    def move(self, direction, walls):\n",
        "        # Both Hunter and Prey can move only UP, DOWN, RIGHT, LEFT, and STAY\n",
        "        x, y = self.position\n",
        "        if direction == 0 and x > 0 and walls[x - 1][y] != \"w\":\n",
        "            x -= 1      # UP\n",
        "        elif direction == 1 and x < self.grid_size - 1 and walls[x + 1][y] != \"w\":\n",
        "            x += 1      # DOWN\n",
        "        elif direction == 2 and y > 0 and walls[x][y - 1] != \"w\":\n",
        "            y -= 1      # LEFT\n",
        "        elif direction == 3 and y < self.grid_size - 1 and walls[x][y + 1] != \"w\":\n",
        "            y += 1      # RIGHT\n",
        "        elif direction == 4:\n",
        "            pass        # STAY\n",
        "        self.position = [x, y]\n",
        "        self.update_vision(walls)\n",
        "\n",
        "    def update_vision(self, walls):\n",
        "        # Vision tool using Bresenham's line algorithm\n",
        "        hx, hy = self.position\n",
        "        self.vision = []\n",
        "\n",
        "        def bresenham(x1, y1, x2, y2):\n",
        "            points = []\n",
        "            dx = abs(x2 - x1)\n",
        "            dy = abs(y2 - y1)\n",
        "            sx = 1 if x1 < x2 else -1\n",
        "            sy = 1 if y1 < y2 else -1\n",
        "            err = dx - dy\n",
        "            while True:\n",
        "                points.append((x1, y1))\n",
        "                if x1 == x2 and y1 == y2:\n",
        "                    break\n",
        "                e2 = 2 * err\n",
        "                if e2 > -dy:\n",
        "                    err -= dy\n",
        "                    x1 += sx\n",
        "                if e2 < dx:\n",
        "                    err += dx\n",
        "                    y1 += sy\n",
        "            return points\n",
        "\n",
        "        for xx in range(self.grid_size):\n",
        "            for yy in range(self.grid_size):\n",
        "                dist = np.sqrt((xx - hx)**2 + (yy - hy)**2)\n",
        "                if dist <= self.fov_radius:\n",
        "                    line = bresenham(hx, hy, xx, yy)\n",
        "                    visible = True\n",
        "                    for lx, ly in line:\n",
        "                        if walls[lx][ly] == \"w\":\n",
        "                            visible = False\n",
        "                            # Local_map: 2=wall\n",
        "                            self.local_map[lx, ly] = 2\n",
        "                            break\n",
        "                        else:\n",
        "                            self.local_map[lx, ly] = 1\n",
        "                    if visible:\n",
        "                        self.vision.append((xx, yy))\n",
        "\n",
        "    def can_see(self, other_position):\n",
        "        return tuple(other_position) in self.vision\n",
        "\n",
        "\n",
        "# ------------------- ENVIRONMENT ------------------- #\n",
        "class Environment:\n",
        "    def __init__(self, grid_size, turns):\n",
        "        self.grid_size = grid_size\n",
        "        self.turns     = turns\n",
        "        self.walls     = self.generate_field(grid_size)\n",
        "\n",
        "        hunter_pos, prey_pos = random.sample(self.accessible_tiles, 2)\n",
        "\n",
        "        wall_map = self.generate_field(grid_size)\n",
        "\n",
        "        while True:\n",
        "            hunter_pos, prey_pos = random.sample(self.accessible_tiles, 2)\n",
        "            if self.check_accessibility(wall_map, hunter_pos, prey_pos):\n",
        "                break\n",
        "\n",
        "        self.hunter = Player(hunter_pos[0], hunter_pos[1], fov_radius=5, grid_size=grid_size)\n",
        "        self.prey   = Player(prey_pos[0],   prey_pos[1],   fov_radius=5, grid_size=grid_size)\n",
        "\n",
        "        self.hunter.update_vision(self.walls)\n",
        "        self.prey.update_vision(self.walls)\n",
        "\n",
        "    def generate_field(self, size):\n",
        "        # Modify p_set to set up what percentage [!walls, walls]\n",
        "        p_set = 0.8\n",
        "        field = np.random.choice([0, 1], size=(size, size), p=[p_set, 1 - p_set])\n",
        "        field[0, :] = 1\n",
        "        field[-1, :] = 1\n",
        "        field[:, 0]  = 1\n",
        "        field[:, -1] = 1\n",
        "\n",
        "        wall_map = np.full((size, size), \".\", dtype=str)\n",
        "        wall_map[field == 1] = \"w\"\n",
        "\n",
        "        self.accessible_tiles = [(x, y) for x in range(size) for y in range(size) if wall_map[x][y] == \".\"]\n",
        "\n",
        "        return wall_map.tolist()\n",
        "\n",
        "    def check_accessibility(self, field, start, end):\n",
        "        queue = [start]\n",
        "        visited = set()\n",
        "        while queue:\n",
        "            x, y = queue.pop(0)\n",
        "            if (x, y) == end:\n",
        "                return True\n",
        "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "                nx, ny = x + dx, y + dy\n",
        "                if 0 <= nx < len(field) and 0 <= ny < len(field[0]):\n",
        "                    if (nx, ny) not in visited and field[nx][ny] == \".\":\n",
        "                        visited.add((nx, ny))\n",
        "                        queue.append((nx, ny))\n",
        "        return False\n",
        "\n",
        "    def get_hunter_state(self):\n",
        "        hx, hy = self.hunter.position\n",
        "        px, py = self.prey.position\n",
        "\n",
        "        # local_map: shape [1, g, g], plus scalar\n",
        "        map_2d = torch.tensor(self.hunter.local_map, dtype=torch.float32).unsqueeze(0)\n",
        "        sees_prey   = 1.0 if self.hunter.can_see(self.prey.position) else 0.0\n",
        "        sees_hunter = 1.0 if self.prey.can_see(self.hunter.position) else 0.0\n",
        "        dx = hx - px\n",
        "        dy = hy - py\n",
        "        scalar = torch.tensor([hx, hy, px, py, sees_prey, sees_hunter, dx, dy], dtype=torch.float32)\n",
        "        return map_2d, scalar\n",
        "\n",
        "    def get_prey_state(self):\n",
        "        hx, hy = self.hunter.position\n",
        "        px, py = self.prey.position\n",
        "        map_2d = torch.tensor(self.prey.local_map, dtype=torch.float32).unsqueeze(0)\n",
        "        sees_hunter = 1.0 if self.prey.can_see(self.hunter.position) else 0.0\n",
        "        sees_prey   = 1.0 if self.hunter.can_see(self.prey.position) else 0.0\n",
        "        dx = px - hx\n",
        "        dy = py - hy\n",
        "        scalar = torch.tensor([px, py, hx, hy, sees_hunter, sees_prey, dx, dy], dtype=torch.float32)\n",
        "        return map_2d, scalar\n",
        "\n",
        "    def step(self, hunter_action, prey_action):\n",
        "        self.hunter.move(hunter_action, self.walls)\n",
        "        self.prey.move(prey_action, self.walls)\n",
        "        dist = a_star_distance(self.walls, tuple(self.hunter.position),\n",
        "                               tuple(self.prey.position), self.grid_size)\n",
        "\n",
        "        if self.hunter.position == self.prey.position:\n",
        "            reward_hunter = 30\n",
        "            reward_prey   = -20.0\n",
        "            done = True\n",
        "        if self.hunter.position != self.prey.position and dist is not None:\n",
        "            reward_hunter = -0.1*dist\n",
        "            reward_prey   = +0.1*dist\n",
        "            done = False\n",
        "        else:\n",
        "            reward_hunter = 0.0\n",
        "            reward_prey = 0.0\n",
        "            done = True\n",
        "\n",
        "        return reward_hunter, reward_prey, done\n",
        "\n",
        "    def render(self, return_frame=False):\n",
        "        grid = [row[:] for row in self.walls]\n",
        "        hx, hy = self.hunter.position\n",
        "        px, py = self.prey.position\n",
        "        grid[hx][hy] = \"H\"\n",
        "        grid[px][py] = \"P\"\n",
        "        if return_frame:\n",
        "            return np.array(grid)\n",
        "        else:\n",
        "            os.system(\"cls\" if os.name == \"nt\" else \"clear\")\n",
        "            print(\"\\n\".join(\" \".join(row) for row in grid))\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "\n",
        "\n",
        "def a_star_distance(walls, start, goal, grid_size):\n",
        "    # Returns the length of the shortest path from 'start' to 'goal' in terms of number of steps\n",
        "    # Or None if there is no path\n",
        "    # If start is the target = 0\n",
        "    if start == goal:\n",
        "        return 0\n",
        "\n",
        "    (sx, sy) = start\n",
        "    (gx, gy) = goal\n",
        "\n",
        "    if walls[sx][sy] == \"w\" or walls[gx][gy] == \"w\":\n",
        "        return None\n",
        "    open_set = []\n",
        "    heapq.heappush(open_set, (0, sx, sy))\n",
        "    came_from = {}\n",
        "    cost_so_far = {(sx, sy): 0}\n",
        "\n",
        "    def heuristic(ax, ay, bx, by):\n",
        "        return abs(ax - bx) + abs(ay - by)\n",
        "\n",
        "    while open_set:\n",
        "        priority, cx, cy = heapq.heappop(open_set)\n",
        "\n",
        "        if (cx, cy) == (gx, gy):\n",
        "            return cost_so_far[(cx, cy)]\n",
        "\n",
        "        for dx, dy in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
        "            nx, ny = cx + dx, cy + dy\n",
        "            if 0 <= nx < grid_size and 0 <= ny < grid_size:\n",
        "                if walls[nx][ny] == \".\":\n",
        "                    new_cost = cost_so_far[(cx, cy)] + 1\n",
        "                    if (nx, ny) not in cost_so_far or new_cost < cost_so_far[(nx, ny)]:\n",
        "                        cost_so_far[(nx, ny)] = new_cost\n",
        "                        priority = new_cost + heuristic(nx, ny, gx, gy)\n",
        "                        heapq.heappush(open_set, (priority, nx, ny))\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "def save_animation(frames, filename, fps=12):\n",
        "    def frame_to_numeric(frame):\n",
        "        mapping = {\n",
        "            \".\": 8,   # Empty space\n",
        "            \"w\": 0,   # Wall\n",
        "            \"H\": 1,   # Hunter\n",
        "            \"P\": 2    # Prey\n",
        "        }\n",
        "        return np.vectorize(mapping.get)(frame)\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Convert each frame into a grid for animation\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        ax.axis(\"off\")\n",
        "        numeric_frame = frame_to_numeric(frame)\n",
        "        ax.imshow(numeric_frame, cmap=\"gray\", vmin=0, vmax=3)\n",
        "\n",
        "    if frames:\n",
        "        frames.extend([frames[-1]] * int(3 * fps))\n",
        "\n",
        "    ani = anime.FuncAnimation(fig, update, frames=frames, interval=1000 / fps)\n",
        "    writer = PillowWriter(fps=fps)\n",
        "    ani.save(filename, writer = writer)\n",
        "    print(f\"Animation saved as {filename}\")\n",
        "\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "def train_hunter(hunter_agent, prey_agent, episodes, grid_size, turns, batch_size, render_on, n_try):\n",
        "    rewards_hunter = []\n",
        "    for episode in range(episodes):\n",
        "        env = Environment(grid_size, turns)\n",
        "        h_map, h_scalar = env.get_hunter_state()\n",
        "        done = False\n",
        "        total_reward_hunter = 0.0\n",
        "        frames = []\n",
        "\n",
        "        for turn in range(turns):\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            hunter_action = hunter_agent.predict(h_map, h_scalar)\n",
        "\n",
        "            p_map, p_scalar = env.get_prey_state()\n",
        "            if len(prey_agent.replay_buffer) < 500:\n",
        "                prey_action = random.randint(0, 4)\n",
        "            else:\n",
        "                prey_action = prey_agent.predict(p_map, p_scalar)\n",
        "\n",
        "            reward_hunter, _, done = env.step(hunter_action, prey_action)\n",
        "\n",
        "            if render_on and episode >= episodes - 5:\n",
        "                    frames.append(env.render(return_frame=True))\n",
        "\n",
        "            h_map_next, h_scalar_next = env.get_hunter_state()\n",
        "            hunter_agent.replay_buffer.push(\n",
        "                h_map, h_scalar,\n",
        "                hunter_action, reward_hunter,\n",
        "                h_map_next, h_scalar_next,\n",
        "                float(done)\n",
        "            )\n",
        "            hunter_agent.train(batch_size)\n",
        "\n",
        "            h_map, h_scalar = h_map_next, h_scalar_next\n",
        "            total_reward_hunter += reward_hunter\n",
        "\n",
        "        hunter_agent.epsilon = max(hunter_agent.epsilon_min, hunter_agent.epsilon * hunter_agent.epsilon_decay)\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            hunter_agent.update_target_model()\n",
        "\n",
        "        rewards_hunter.append(total_reward_hunter)\n",
        "        print(f\"Episode {episode+1} out of {episodes} (Hunter)\")\n",
        "\n",
        "        if render_on and episode >= episodes - 5:\n",
        "            save_animation(frames, f\"hunter_episode_{n_try+1}_{episode+1}.gif\")\n",
        "\n",
        "    return rewards_hunter\n",
        "\n",
        "\n",
        "\n",
        "def train_prey(prey_agent, hunter_agent, episodes, grid_size, turns, batch_size, render_on, n_try):\n",
        "    rewards_prey = []\n",
        "    for episode in range(episodes):\n",
        "        env = Environment(grid_size, turns)\n",
        "        p_map, p_scalar = env.get_prey_state()\n",
        "        done = False\n",
        "        total_reward_prey = 0.0\n",
        "        frames = []\n",
        "\n",
        "        for turn in range(turns):\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            h_map, h_scalar = env.get_hunter_state()\n",
        "            hunter_action   = hunter_agent.predict(h_map, h_scalar)\n",
        "\n",
        "            prey_action = prey_agent.predict(p_map, p_scalar)\n",
        "            _, reward_prey, done = env.step(hunter_action, prey_action)\n",
        "\n",
        "            if render_on and episode >= episodes - 5:\n",
        "                    frames.append(env.render(return_frame=True))\n",
        "\n",
        "\n",
        "            p_map_next, p_scalar_next = env.get_prey_state()\n",
        "            prey_agent.replay_buffer.push(\n",
        "                p_map, p_scalar,\n",
        "                prey_action, reward_prey,\n",
        "                p_map_next, p_scalar_next,\n",
        "                float(done)\n",
        "            )\n",
        "            prey_agent.train(batch_size)\n",
        "\n",
        "            p_map, p_scalar = p_map_next, p_scalar_next\n",
        "            total_reward_prey += reward_prey\n",
        "\n",
        "        prey_agent.epsilon = max(prey_agent.epsilon_min, prey_agent.epsilon * prey_agent.epsilon_decay)\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            prey_agent.update_target_model()\n",
        "\n",
        "        rewards_prey.append(total_reward_prey)\n",
        "        print(f\"Episode {episode+1} out of {episodes} (Prey)\")\n",
        "\n",
        "        if render_on and episode >= episodes - 5:\n",
        "            save_animation(frames, f\"prey_episode_{n_try+1}_{episode+1}.gif\")\n",
        "\n",
        "    return rewards_prey\n",
        "\n",
        "\n",
        "def train_IQL(hunter_agent, prey_agent, episodes_hunter, episodes_prey, grid_size, turns, batch_size, tries, render_on):\n",
        "    total_reward_hunter = []\n",
        "    total_reward_prey   = []\n",
        "    for n_try in range(tries):\n",
        "        print(f\"=== Switching sides! Hunter's turn {n_try+1} ===\")\n",
        "        rh = train_hunter(hunter_agent, prey_agent, episodes_hunter,\n",
        "                          grid_size, turns, batch_size, render_on, n_try)\n",
        "        total_reward_hunter.extend(rh)\n",
        "\n",
        "        print(f\"=== Switching sides! Prey's turn {n_try+1} ===\")\n",
        "        rp = train_prey(prey_agent, hunter_agent, episodes_prey,\n",
        "                        grid_size, turns, batch_size, render_on, n_try)\n",
        "        total_reward_prey.extend(rp)\n",
        "\n",
        "    # MatPlotLib graphic output of the training cycle conducted\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    # Plot Hunter rewards\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(range(len(total_reward_hunter)), total_reward_hunter, label=\"Hunter\", color='#0E0598', s=5)\n",
        "    avg_hunter = [np.mean(total_reward_hunter[max(0, i-50):i+1]) for i in range(len(total_reward_hunter))]\n",
        "    plt.plot(range(len(total_reward_hunter)), avg_hunter, color='orange', label=\"Hunter Avg (50)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.title(f\"IQL Hunter:\\nGrid {grid_size}x{grid_size}; Turns {turns}; Episodes {episodes_hunter*tries*2}; FOV 5\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    # Plot Prey rewards\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(range(len(total_reward_prey)), total_reward_prey, label=\"Prey\", color='xkcd:baby poop green', s=5)\n",
        "    rolling_avg_prey = [np.mean(total_reward_prey[max(0, i-50):i+1]) for i in range(len(total_reward_prey))]\n",
        "    plt.plot(range(len(total_reward_prey)), rolling_avg_prey, color='green', label=\"Prey Avg (50)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.title(f\"IQL Prey:\\nGrid {grid_size}x{grid_size}; Turns {turns}; Episodes {episodes_hunter*tries*2}; FOV 5\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # === SETTINGS === #\n",
        "    #\n",
        "    # grid_size                      -- set up the grid size of the map.\n",
        "    #                                   Do not compensate for walls (+2), they are not accounted for in the logic\n",
        "    # episodes_hunter, episodes_prey -- Setup Hunter and Prey episodes for their unique\n",
        "    #                                   trainings (SHOULD BE EQUAL)\n",
        "    # turns                          -- accounted for automatically\n",
        "    # batch size                     -- setup the batch size\n",
        "    # tries                          -- the number of cycles both of the models train\n",
        "    # render_on                      -- if you want to turn the render of the field on to see the process of training\n",
        "    #                                   May significantly impact the speed of the training\n",
        "    grid_size = 12\n",
        "    hunter_agent = Agent(grid_size=grid_size, scalar_size=8, n_actions=5)\n",
        "    prey_agent   = Agent(grid_size=grid_size, scalar_size=8, n_actions=5)\n",
        "\n",
        "    train_IQL(hunter_agent, prey_agent,\n",
        "              episodes_hunter=1000,\n",
        "              episodes_prey=1000,\n",
        "              grid_size=grid_size,\n",
        "              turns=int(grid_size*grid_size*2),\n",
        "              batch_size=32,\n",
        "              tries=3,\n",
        "              render_on=True)"
      ]
    }
  ]
}
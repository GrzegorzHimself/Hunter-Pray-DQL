{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoNOZEucAY+xYpnVeHfV61",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrzegorzHimself/Hunter-Prey-IQL/blob/main/HnP_8_inputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qUmqCcgAWvn",
        "outputId": "f58ecbe3-56b0-4de1-e224-6b89c772accc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Switching sides! Hunter's turn 1 ===\n",
            "Episode 1 out of 1000\n",
            "Episode 2 out of 1000\n",
            "Episode 3 out of 1000\n",
            "Episode 4 out of 1000\n",
            "Episode 5 out of 1000\n",
            "Episode 6 out of 1000\n",
            "Episode 7 out of 1000\n",
            "Episode 8 out of 1000\n",
            "Episode 9 out of 1000\n",
            "Episode 10 out of 1000\n",
            "Episode 11 out of 1000\n",
            "Episode 12 out of 1000\n",
            "Episode 13 out of 1000\n",
            "Episode 14 out of 1000\n",
            "Episode 15 out of 1000\n",
            "Episode 16 out of 1000\n",
            "Episode 17 out of 1000\n",
            "Episode 18 out of 1000\n",
            "Episode 19 out of 1000\n",
            "Episode 20 out of 1000\n",
            "Episode 21 out of 1000\n",
            "Episode 22 out of 1000\n",
            "Episode 23 out of 1000\n",
            "Episode 24 out of 1000\n",
            "Episode 25 out of 1000\n",
            "Episode 26 out of 1000\n",
            "Episode 27 out of 1000\n",
            "Episode 28 out of 1000\n",
            "Episode 29 out of 1000\n",
            "Episode 30 out of 1000\n",
            "Episode 31 out of 1000\n",
            "Episode 32 out of 1000\n",
            "Episode 33 out of 1000\n",
            "Episode 34 out of 1000\n",
            "Episode 35 out of 1000\n",
            "Episode 36 out of 1000\n",
            "Episode 37 out of 1000\n",
            "Episode 38 out of 1000\n",
            "Episode 39 out of 1000\n",
            "Episode 40 out of 1000\n",
            "Episode 41 out of 1000\n",
            "Episode 42 out of 1000\n",
            "Episode 43 out of 1000\n",
            "Episode 44 out of 1000\n",
            "Episode 45 out of 1000\n",
            "Episode 46 out of 1000\n",
            "Episode 47 out of 1000\n",
            "Episode 48 out of 1000\n",
            "Episode 49 out of 1000\n",
            "Episode 50 out of 1000\n",
            "Episode 51 out of 1000\n",
            "Episode 52 out of 1000\n",
            "Episode 53 out of 1000\n",
            "Episode 54 out of 1000\n",
            "Episode 55 out of 1000\n",
            "Episode 56 out of 1000\n",
            "Episode 57 out of 1000\n",
            "Episode 58 out of 1000\n",
            "Episode 59 out of 1000\n",
            "Episode 60 out of 1000\n",
            "Episode 61 out of 1000\n",
            "Episode 62 out of 1000\n",
            "Episode 63 out of 1000\n",
            "Episode 64 out of 1000\n",
            "Episode 65 out of 1000\n",
            "Episode 66 out of 1000\n",
            "Episode 67 out of 1000\n",
            "Episode 68 out of 1000\n",
            "Episode 69 out of 1000\n",
            "Episode 70 out of 1000\n",
            "Episode 71 out of 1000\n",
            "Episode 72 out of 1000\n",
            "Episode 73 out of 1000\n",
            "Episode 74 out of 1000\n",
            "Episode 75 out of 1000\n",
            "Episode 76 out of 1000\n",
            "Episode 77 out of 1000\n",
            "Episode 78 out of 1000\n",
            "Episode 79 out of 1000\n",
            "Episode 80 out of 1000\n",
            "Episode 81 out of 1000\n",
            "Episode 82 out of 1000\n",
            "Episode 83 out of 1000\n",
            "Episode 84 out of 1000\n",
            "Episode 85 out of 1000\n",
            "Episode 86 out of 1000\n",
            "Episode 87 out of 1000\n",
            "Episode 88 out of 1000\n",
            "Episode 89 out of 1000\n",
            "Episode 90 out of 1000\n",
            "Episode 91 out of 1000\n",
            "Episode 92 out of 1000\n",
            "Episode 93 out of 1000\n",
            "Episode 94 out of 1000\n",
            "Episode 95 out of 1000\n",
            "Episode 96 out of 1000\n",
            "Episode 97 out of 1000\n",
            "Episode 98 out of 1000\n",
            "Episode 99 out of 1000\n",
            "Episode 100 out of 1000\n",
            "Episode 101 out of 1000\n",
            "Episode 102 out of 1000\n",
            "Episode 103 out of 1000\n",
            "Episode 104 out of 1000\n",
            "Episode 105 out of 1000\n",
            "Episode 106 out of 1000\n",
            "Episode 107 out of 1000\n",
            "Episode 108 out of 1000\n",
            "Episode 109 out of 1000\n",
            "Episode 110 out of 1000\n",
            "Episode 111 out of 1000\n",
            "Episode 112 out of 1000\n",
            "Episode 113 out of 1000\n",
            "Episode 114 out of 1000\n",
            "Episode 115 out of 1000\n",
            "Episode 116 out of 1000\n",
            "Episode 117 out of 1000\n",
            "Episode 118 out of 1000\n",
            "Episode 119 out of 1000\n",
            "Episode 120 out of 1000\n",
            "Episode 121 out of 1000\n",
            "Episode 122 out of 1000\n",
            "Episode 123 out of 1000\n",
            "Episode 124 out of 1000\n",
            "Episode 125 out of 1000\n",
            "Episode 126 out of 1000\n",
            "Episode 127 out of 1000\n",
            "Episode 128 out of 1000\n",
            "Episode 129 out of 1000\n",
            "Episode 130 out of 1000\n",
            "Episode 131 out of 1000\n",
            "Episode 132 out of 1000\n",
            "Episode 133 out of 1000\n",
            "Episode 134 out of 1000\n",
            "Episode 135 out of 1000\n",
            "Episode 136 out of 1000\n",
            "Episode 137 out of 1000\n",
            "Episode 138 out of 1000\n",
            "Episode 139 out of 1000\n",
            "Episode 140 out of 1000\n",
            "Episode 141 out of 1000\n",
            "Episode 142 out of 1000\n",
            "Episode 143 out of 1000\n",
            "Episode 144 out of 1000\n",
            "Episode 145 out of 1000\n",
            "Episode 146 out of 1000\n",
            "Episode 147 out of 1000\n",
            "Episode 148 out of 1000\n",
            "Episode 149 out of 1000\n",
            "Episode 150 out of 1000\n",
            "Episode 151 out of 1000\n",
            "Episode 152 out of 1000\n",
            "Episode 153 out of 1000\n",
            "Episode 154 out of 1000\n",
            "Episode 155 out of 1000\n",
            "Episode 156 out of 1000\n",
            "Episode 157 out of 1000\n",
            "Episode 158 out of 1000\n",
            "Episode 159 out of 1000\n",
            "Episode 160 out of 1000\n",
            "Episode 161 out of 1000\n",
            "Episode 162 out of 1000\n",
            "Episode 163 out of 1000\n",
            "Episode 164 out of 1000\n",
            "Episode 165 out of 1000\n",
            "Episode 166 out of 1000\n",
            "Episode 167 out of 1000\n",
            "Episode 168 out of 1000\n",
            "Episode 169 out of 1000\n",
            "Episode 170 out of 1000\n",
            "Episode 171 out of 1000\n",
            "Episode 172 out of 1000\n",
            "Episode 173 out of 1000\n",
            "Episode 174 out of 1000\n",
            "Episode 175 out of 1000\n",
            "Episode 176 out of 1000\n",
            "Episode 177 out of 1000\n",
            "Episode 178 out of 1000\n",
            "Episode 179 out of 1000\n",
            "Episode 180 out of 1000\n",
            "Episode 181 out of 1000\n",
            "Episode 182 out of 1000\n",
            "Episode 183 out of 1000\n",
            "Episode 184 out of 1000\n",
            "Episode 185 out of 1000\n",
            "Episode 186 out of 1000\n",
            "Episode 187 out of 1000\n",
            "Episode 188 out of 1000\n",
            "Episode 189 out of 1000\n",
            "Episode 190 out of 1000\n",
            "Episode 191 out of 1000\n",
            "Episode 192 out of 1000\n",
            "Episode 193 out of 1000\n",
            "Episode 194 out of 1000\n",
            "Episode 195 out of 1000\n",
            "Episode 196 out of 1000\n",
            "Episode 197 out of 1000\n",
            "Episode 198 out of 1000\n",
            "Episode 199 out of 1000\n",
            "Episode 200 out of 1000\n",
            "Episode 201 out of 1000\n",
            "Episode 202 out of 1000\n",
            "Episode 203 out of 1000\n",
            "Episode 204 out of 1000\n",
            "Episode 205 out of 1000\n",
            "Episode 206 out of 1000\n",
            "Episode 207 out of 1000\n",
            "Episode 208 out of 1000\n",
            "Episode 209 out of 1000\n",
            "Episode 210 out of 1000\n",
            "Episode 211 out of 1000\n",
            "Episode 212 out of 1000\n",
            "Episode 213 out of 1000\n",
            "Episode 214 out of 1000\n",
            "Episode 215 out of 1000\n",
            "Episode 216 out of 1000\n",
            "Episode 217 out of 1000\n",
            "Episode 218 out of 1000\n",
            "Episode 219 out of 1000\n",
            "Episode 220 out of 1000\n",
            "Episode 221 out of 1000\n",
            "Episode 222 out of 1000\n",
            "Episode 223 out of 1000\n",
            "Episode 224 out of 1000\n",
            "Episode 225 out of 1000\n",
            "Episode 226 out of 1000\n",
            "Episode 227 out of 1000\n",
            "Episode 228 out of 1000\n",
            "Episode 229 out of 1000\n",
            "Episode 230 out of 1000\n",
            "Episode 231 out of 1000\n",
            "Episode 232 out of 1000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as anime\n",
        "from matplotlib.animation import PillowWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- REPLAY BUFFER ------------------- #\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        # Replay buffer setup for shuffle and mini-batch creation\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
        "            torch.tensor(actions, dtype=torch.long).to(device),\n",
        "            torch.tensor(rewards, dtype=torch.float32).to(device),\n",
        "            torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
        "            torch.tensor(dones, dtype=torch.float32).to(device),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- DQN NETWORK ------------------- #\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))     # I love ReLU\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- AGENT ------------------- #\n",
        "class Agent:\n",
        "    def __init__(self, input_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, lr=0.001):\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.model = DQN(input_dim, n_actions).to(device)\n",
        "        self.target_model = DQN(input_dim, n_actions).to(device)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.target_model.eval()\n",
        "# Nice\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.replay_buffer = ReplayBuffer(10000)\n",
        "\n",
        "    def predict(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            # Random action, exploration\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        else:\n",
        "            # Exploitation\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "                q_values = self.model(state_tensor)\n",
        "                return torch.argmax(q_values).item()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def train(self, batch_size):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        # Current Q-values\n",
        "        q_values = self.model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Next Q-values\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_model(next_states).max(1)[0]\n",
        "\n",
        "        # Target Q-values\n",
        "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        # Loss\n",
        "        loss = self.criterion(q_values, target_q_values)\n",
        "\n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- PLAYER ------------------- #\n",
        "class Player:\n",
        "    def __init__(self, x, y, fov_radius, grid_size):\n",
        "        self.position = [x, y]\n",
        "        self.fov_radius = fov_radius\n",
        "        self.grid_size = grid_size\n",
        "        self.vision = []\n",
        "\n",
        "    def move(self, direction, walls):\n",
        "        # Both Hunter and Prey can move only UP, DOWN, RIGHT, LEFT, and STAY (in place, basically skipping the step)\n",
        "        x, y = self.position\n",
        "        if direction == 0 and x > 0 and walls[x - 1][y] != \"w\":\n",
        "            x -= 1      # UP\n",
        "        elif direction == 1 and x < self.grid_size - 1 and walls[x + 1][y] != \"w\":\n",
        "            x += 1      # DOWN\n",
        "        elif direction == 2 and y > 0 and walls[x][y - 1] != \"w\":\n",
        "            y -= 1      # LEFT\n",
        "        elif direction == 3 and y < self.grid_size - 1 and walls[x][y + 1] != \"w\":\n",
        "            y += 1      # RIGHT\n",
        "        elif direction == 4:\n",
        "            pass        # STAY\n",
        "        self.position = [x, y]\n",
        "        self.update_vision(walls)\n",
        "\n",
        "    # Vision tool using Bresenham's line algorithm\n",
        "    # Provides Hunter and Pray with a FOV (Field Of View) circle\n",
        "    # In this circle, they \"see\" each other (see *move* function for details)\n",
        "    # Walls block the circle, thus limiting the FOV\n",
        "    def update_vision(self, walls):\n",
        "        hx, hy = self.position\n",
        "        self.vision = []\n",
        "\n",
        "        def bresenham(x1, y1, x2, y2):\n",
        "            points = []\n",
        "            dx = abs(x2 - x1)\n",
        "            dy = abs(y2 - y1)\n",
        "            sx = 1 if x1 < x2 else -1\n",
        "            sy = 1 if y1 < y2 else -1\n",
        "            err = dx - dy\n",
        "\n",
        "            while True:\n",
        "                points.append((x1, y1))\n",
        "                if x1 == x2 and y1 == y2:\n",
        "                    break\n",
        "                e2 = 2 * err\n",
        "                if e2 > -dy:\n",
        "                    err -= dy\n",
        "                    x1 += sx\n",
        "                if e2 < dx:\n",
        "                    err += dx\n",
        "                    y1 += sy\n",
        "\n",
        "            return points\n",
        "\n",
        "        for x in range(self.grid_size):\n",
        "            for y in range(self.grid_size):\n",
        "                if np.sqrt((x - hx) ** 2 + (y - hy) ** 2) <= self.fov_radius:\n",
        "                    line = bresenham(hx, hy, x, y)\n",
        "                    visible = True\n",
        "                    for lx, ly in line:\n",
        "                        if walls[lx][ly] == \"w\":\n",
        "                            visible = False\n",
        "                            break\n",
        "                    if visible:\n",
        "                        self.vision.append((lx, ly))\n",
        "\n",
        "    def can_see(self, other_position):\n",
        "        return tuple(other_position) in self.vision\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- ENVIRONMENT ------------------- #\n",
        "class Environment:\n",
        "    def __init__(self, grid_size, turns):\n",
        "        # Declaring class for the game itself\n",
        "        # Generates field, checks for movement opportunities\n",
        "        # Follows the state of each object and renders field\n",
        "        self.grid_size = grid_size\n",
        "        self.turns = turns\n",
        "        self.walls = self.generate_field(grid_size)\n",
        "\n",
        "        hunter_pos, prey_pos = random.sample(self.accessible_tiles, 2)\n",
        "\n",
        "        self.hunter = Player(hunter_pos[0], hunter_pos[1], fov_radius=5, grid_size=grid_size)\n",
        "        self.prey = Player(prey_pos[0], prey_pos[1], fov_radius=5, grid_size=grid_size)\n",
        "\n",
        "        self.hunter.update_vision(self.walls)\n",
        "        self.prey.update_vision(self.walls)\n",
        "\n",
        "    def generate_field(self, size):\n",
        "        # Modify p_set to set up what percentage [!walls, walls]\n",
        "        p_set = 0.8\n",
        "        field = np.random.choice([0, 1], size=(size, size), p=[p_set, 1.0-p_set])\n",
        "        field[0, :] = 1\n",
        "        field[-1, :] = 1\n",
        "        field[:, 0] = 1\n",
        "        field[:, -1] = 1\n",
        "\n",
        "        wall_map = np.full((size, size), \".\", dtype=str)\n",
        "        wall_map[field == 1] = \"w\"\n",
        "\n",
        "        self.accessible_tiles = [(x, y) for x in range(size) for y in range(size) if wall_map[x][y] == \".\"]\n",
        "\n",
        "        while True:\n",
        "            hunter_pos, prey_pos = random.sample(self.accessible_tiles, 2)\n",
        "            if self.check_accessibility(wall_map, hunter_pos, prey_pos):\n",
        "                break\n",
        "\n",
        "        # The only function here  to tweak is FOV radius, uniquely for each player\n",
        "        self.hunter = Player(hunter_pos[0], hunter_pos[1], fov_radius=5, grid_size=size)\n",
        "        self.prey   = Player(prey_pos[0],   prey_pos[1],   fov_radius=5, grid_size=size)\n",
        "        return wall_map.tolist()\n",
        "\n",
        "    def check_accessibility(self, field, start, end):\n",
        "        queue = [start]\n",
        "        visited = set()\n",
        "        while queue:\n",
        "            x, y = queue.pop(0)\n",
        "            if (x, y) == end:\n",
        "                return True\n",
        "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "                nx, ny = x + dx, y + dy\n",
        "                if 0 <= nx < len(field) and 0 <= ny < len(field[0]) and (nx, ny) not in visited and field[nx][ny] == \".\":\n",
        "                    queue.append((nx, ny))\n",
        "                    visited.add((nx, ny))\n",
        "        return False\n",
        "\n",
        "    def get_state(self):\n",
        "        hunter_x, hunter_y = self.hunter.position\n",
        "        prey_x, prey_y = self.prey.position\n",
        "        hunter_sees_prey = 1 if self.hunter.can_see(self.prey.position) else 0\n",
        "        prey_sees_hunter = 1 if self.prey.can_see(self.hunter.position) else 0\n",
        "        dx = hunter_x - prey_x\n",
        "        dy = hunter_y - prey_y\n",
        "        return np.array([hunter_x, hunter_y, prey_x, prey_y, hunter_sees_prey, prey_sees_hunter, dx, dy], dtype=np.float32)\n",
        "\n",
        "    def step(self, hunter_action, prey_action):\n",
        "        self.hunter.move(hunter_action, self.walls)\n",
        "        self.prey.move(prey_action, self.walls)\n",
        "\n",
        "        if self.hunter.position == self.prey.position:\n",
        "            reward_hunter = +10.0\n",
        "            reward_prey   = -10.0\n",
        "            done = True\n",
        "        else:\n",
        "            reward_hunter = -0.1\n",
        "            reward_prey   = +0.1\n",
        "            done = False\n",
        "\n",
        "        return self.get_state(), reward_hunter, reward_prey, done\n",
        "\n",
        "    def render(self, return_frame=False):\n",
        "        grid = [row[:] for row in self.walls]\n",
        "        hx, hy = self.hunter.position\n",
        "        px, py = self.prey.position\n",
        "        grid[hx][hy] = \"H\"\n",
        "        grid[px][py] = \"P\"\n",
        "        if return_frame:\n",
        "            return np.array(grid)\n",
        "        else:\n",
        "            print(\"\\n\".join(\" \".join(row) for row in grid))\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "\n",
        "\n",
        "def save_animation(frames, filename, fps=12):\n",
        "    def frame_to_numeric(frame):\n",
        "        mapping = {\n",
        "            \".\": 8,   # Empty space\n",
        "            \"w\": 0,   # Wall\n",
        "            \"H\": 1,   # Hunter\n",
        "            \"P\": 2    # Prey\n",
        "        }\n",
        "        return np.vectorize(mapping.get)(frame)\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Convert each frame into a grid animation can process\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        ax.axis(\"off\")\n",
        "        numeric_frame = frame_to_numeric(frame)\n",
        "        ax.imshow(numeric_frame, cmap=\"gray\", vmin=0, vmax=3)\n",
        "\n",
        "    if frames:\n",
        "        frames.extend([frames[-1]] * int(3 * fps))\n",
        "\n",
        "    ani = anime.FuncAnimation(fig, update, frames=frames, interval=1000 / fps)\n",
        "    writer = PillowWriter(fps=fps)\n",
        "    ani.save(filename, writer = writer)\n",
        "    print(f\"Animation saved as {filename}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_hunter(hunter_agent, prey_agent, episodes, grid_size, turns, batch_size, render_on, n_try):\n",
        "    rewards_hunter = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        env = Environment(grid_size, turns)\n",
        "        state = env.get_state()\n",
        "        done = False\n",
        "        total_reward_hunter = 0.0\n",
        "        frames = []\n",
        "\n",
        "        for turn in range(turns):\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            hunter_action = hunter_agent.predict(state)\n",
        "            if len(prey_agent.replay_buffer) < 500:\n",
        "                prey_action = random.randint(0, 4)\n",
        "            else:\n",
        "                prey_action = prey_agent.predict(state)\n",
        "\n",
        "            next_state, reward_hunter, _, done = env.step(hunter_action, prey_action)\n",
        "\n",
        "            # Render the step\n",
        "            if render_on and episode >= episodes - 5:\n",
        "                    frames.append(env.render(return_frame=True))\n",
        "\n",
        "            hunter_agent.replay_buffer.push(\n",
        "                state, hunter_action, reward_hunter, next_state, done\n",
        "            )\n",
        "\n",
        "            hunter_agent.train(batch_size)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward_hunter += reward_hunter\n",
        "\n",
        "        hunter_agent.epsilon = max(hunter_agent.epsilon_min, hunter_agent.epsilon * hunter_agent.epsilon_decay)\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            hunter_agent.update_target_model()\n",
        "\n",
        "        rewards_hunter.append(total_reward_hunter)\n",
        "\n",
        "        print(f\"Episode {episode+1} out of {episodes}\")\n",
        "\n",
        "        if render_on and episode >= episodes - 5:\n",
        "            save_animation(frames, f\"hunter_episode_{n_try+1}_{episode+1}.gif\")\n",
        "\n",
        "    return rewards_hunter\n",
        "\n",
        "\n",
        "\n",
        "def train_prey(prey_agent, hunter_agent, episodes, grid_size, turns, batch_size, render_on, n_try):\n",
        "    rewards_prey = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        env = Environment(grid_size, turns)\n",
        "        state = env.get_state()\n",
        "        done = False\n",
        "        total_reward_prey = 0.0\n",
        "        frames = []\n",
        "\n",
        "        for turn in range(turns):\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            hunter_action = hunter_agent.predict(state)\n",
        "            prey_action   = prey_agent.predict(state)\n",
        "\n",
        "            next_state, _, reward_prey, done = env.step(hunter_action, prey_action)\n",
        "            prey_agent.replay_buffer.push(\n",
        "                state, prey_action, reward_prey, next_state, done\n",
        "            )\n",
        "\n",
        "            # Render the step\n",
        "            if render_on and episode >= episodes - 5:\n",
        "                    frames.append(env.render(return_frame=True))\n",
        "\n",
        "            prey_agent.train(batch_size)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward_prey += reward_prey\n",
        "\n",
        "        prey_agent.epsilon = max(prey_agent.epsilon_min, prey_agent.epsilon * prey_agent.epsilon_decay)\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            prey_agent.update_target_model()\n",
        "\n",
        "        rewards_prey.append(total_reward_prey)\n",
        "        print(f\"Episode {episode+1} out of {episodes}\")\n",
        "\n",
        "        if render_on and episode >= episodes - 5:\n",
        "            save_animation(frames, f\"prey_episode_{n_try+1}_{episode+1}.gif\")\n",
        "\n",
        "    return rewards_prey\n",
        "\n",
        "\n",
        "\n",
        "def train_IQL(hunter_agent, prey_agent, episodes_hunter, episodes_prey, grid_size, turns, batch_size, tries, render_on):\n",
        "    total_reward_hunter = []\n",
        "    total_reward_prey   = []\n",
        "    for n_try in range(tries):\n",
        "        print(f\"=== Switching sides! Hunter's turn {n_try+1} ===\")\n",
        "        rewards_hunter = train_hunter(hunter_agent, prey_agent, episodes_hunter, grid_size, turns, batch_size, render_on, n_try)\n",
        "        total_reward_hunter.extend(rewards_hunter)\n",
        "\n",
        "        print(f\"=== Switching sides! Prey's turn {n_try+1}! ===\")\n",
        "        rewards_prey = train_prey(prey_agent, hunter_agent, episodes_prey, grid_size, turns, batch_size, render_on, n_try)\n",
        "        total_reward_prey.extend(rewards_prey)\n",
        "\n",
        "    # MatPlotLib graphic output of the training cycle conducted\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    # Plot Hunter rewards\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(range(len(total_reward_hunter)), total_reward_hunter, label=\"Hunter\", color='#0E0598', s=10)\n",
        "    avg_hunter = [np.mean(total_reward_hunter[max(0, i-50):i+1]) for i in range(len(total_reward_hunter))]\n",
        "    plt.plot(range(len(total_reward_hunter)), avg_hunter, color='orange', label=\"Hunter Avg (50)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.title(f\"IQL Hunter:\\nGrid {grid_size}x{grid_size}; Turns {turns}; Episodes {episodes_hunter*tries*2}; FOV 5\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    # Plot Prey rewards\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(range(len(total_reward_prey)), total_reward_prey, label=\"Prey\", color='xkcd:baby poop green', s=10)\n",
        "    rolling_avg_prey = [np.mean(total_reward_prey[max(0, i-50):i+1]) for i in range(len(total_reward_prey))]\n",
        "    plt.plot(range(len(total_reward_prey)), rolling_avg_prey, color='green', label=\"Prey Avg (50)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.title(f\"IQL Prey:\\nGrid {grid_size}x{grid_size}; Turns {turns}; Episodes {episodes_hunter*tries*2}; FOV 5\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    grid_size = 12\n",
        "    hunter_agent = Agent(input_dim=8, n_actions=5)\n",
        "    prey_agent   = Agent(input_dim=8, n_actions=5)\n",
        "\n",
        "    # === SETTINGS === #\n",
        "    #\n",
        "    # episodes_hunter, episodes_prey -- Setup Hunter and Prey episodes for their unique\n",
        "    #                                   trainings (SHOULD BE EQUAL)\n",
        "    # grid_size                      -- set up the grid size of the map.\n",
        "    #                                   Do not compensate for walls (+2), they are not accounted for in the logic\n",
        "    # turns                          -- accounted for automatically\n",
        "    # batch size                     -- setup the batch size\n",
        "    # tries                          -- the number of cycles both of the models train\n",
        "    # render_on                      -- if you want to turn the render of the field on to see the process of training\n",
        "    #                                   May significantly impact the speed of the training\n",
        "    train_IQL(hunter_agent, prey_agent,\n",
        "              episodes_hunter=1000,\n",
        "              episodes_prey=1000,\n",
        "              grid_size=grid_size,\n",
        "              turns=int(grid_size*grid_size*0.75),\n",
        "              batch_size=32,\n",
        "              tries = 6,\n",
        "              render_on = False)"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMS2q1+zbLGYBuADwV+UkMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrzegorzHimself/Hunter-Prey-IQL/blob/main/HnP_IQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qUmqCcgAWvn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as anime\n",
        "from matplotlib.animation import PillowWriter\n",
        "import torch.nn.functional as F\n",
        "import heapq\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- REPLAY BUFFER ------------------- #\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        # Replay buffer setup for shuffle and mini-batch creation\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
        "            torch.tensor(actions, dtype=torch.long).to(device),\n",
        "            torch.tensor(rewards, dtype=torch.float32).to(device),\n",
        "            torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
        "            torch.tensor(dones, dtype=torch.float32).to(device),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- DQN NETWORK ------------------- #\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))     # I love ReLU\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- AGENT ------------------- #\n",
        "class Agent:\n",
        "    def __init__(self, input_dim, n_actions, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, lr=0.001):\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.model = DQN(input_dim, n_actions).to(device)\n",
        "        self.target_model = DQN(input_dim, n_actions).to(device)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.target_model.eval()\n",
        "# Nice\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.replay_buffer = ReplayBuffer(10000)\n",
        "\n",
        "    def predict(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            # Random action, exploration\n",
        "            return random.randint(0, self.n_actions - 1)\n",
        "        else:\n",
        "            # Exploitation\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "                q_values = self.model(state_tensor)\n",
        "                return torch.argmax(q_values).item()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def train(self, batch_size):\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "        # Current Q-values\n",
        "        q_values = self.model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Next Q-values\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_model(next_states).max(1)[0]\n",
        "\n",
        "        # Target Q-values\n",
        "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        # Loss\n",
        "        loss = self.criterion(q_values, target_q_values)\n",
        "\n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- PLAYER ------------------- #\n",
        "class Player:\n",
        "    def __init__(self, x, y, fov_radius, grid_size):\n",
        "        self.position = [x, y]\n",
        "        self.fov_radius = fov_radius\n",
        "        self.grid_size = grid_size\n",
        "        self.vision = []\n",
        "\n",
        "    def move(self, direction, walls):\n",
        "        # Both Hunter and Prey can move only UP, DOWN, RIGHT, LEFT, and STAY (in place, basically skipping the step)\n",
        "        x, y = self.position\n",
        "        if direction == 0 and x > 0 and walls[x - 1][y] != \"w\":\n",
        "            x -= 1      # UP\n",
        "        elif direction == 1 and x < self.grid_size - 1 and walls[x + 1][y] != \"w\":\n",
        "            x += 1      # DOWN\n",
        "        elif direction == 2 and y > 0 and walls[x][y - 1] != \"w\":\n",
        "            y -= 1      # LEFT\n",
        "        elif direction == 3 and y < self.grid_size - 1 and walls[x][y + 1] != \"w\":\n",
        "            y += 1      # RIGHT\n",
        "        elif direction == 4:\n",
        "            pass        # STAY\n",
        "        self.position = [x, y]\n",
        "        self.update_vision(walls)\n",
        "\n",
        "    # Vision tool using Bresenham's line algorithm\n",
        "    # Provides Hunter and Pray with a FOV (Field Of View) circle\n",
        "    # In this circle, they \"see\" each other (see *move* function for details)\n",
        "    # Walls block the circle, thus limiting the FOV\n",
        "    def update_vision(self, walls):\n",
        "        hx, hy = self.position\n",
        "        self.vision = []\n",
        "\n",
        "        def bresenham(x1, y1, x2, y2):\n",
        "            points = []\n",
        "            dx = abs(x2 - x1)\n",
        "            dy = abs(y2 - y1)\n",
        "            sx = 1 if x1 < x2 else -1\n",
        "            sy = 1 if y1 < y2 else -1\n",
        "            err = dx - dy\n",
        "\n",
        "            while True:\n",
        "                points.append((x1, y1))\n",
        "                if x1 == x2 and y1 == y2:\n",
        "                    break\n",
        "                e2 = 2 * err\n",
        "                if e2 > -dy:\n",
        "                    err -= dy\n",
        "                    x1 += sx\n",
        "                if e2 < dx:\n",
        "                    err += dx\n",
        "                    y1 += sy\n",
        "\n",
        "            return points\n",
        "\n",
        "        for x in range(self.grid_size):\n",
        "            for y in range(self.grid_size):\n",
        "                if np.sqrt((x - hx) ** 2 + (y - hy) ** 2) <= self.fov_radius:\n",
        "                    line = bresenham(hx, hy, x, y)\n",
        "                    visible = True\n",
        "                    for lx, ly in line:\n",
        "                        if walls[lx][ly] == \"w\":\n",
        "                            visible = False\n",
        "                            break\n",
        "                    if visible:\n",
        "                        self.vision.append((lx, ly))\n",
        "\n",
        "    def can_see(self, other_position):\n",
        "        return tuple(other_position) in self.vision\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- ENVIRONMENT ------------------- #\n",
        "class Environment:\n",
        "    def __init__(self, grid_size, turns):\n",
        "        self.grid_size = grid_size\n",
        "        self.turns     = turns\n",
        "        self.walls     = self.generate_field(grid_size)\n",
        "\n",
        "        hunter_pos, prey_pos = random.sample(self.accessible_tiles, 2)\n",
        "\n",
        "        wall_map = self.generate_field(grid_size)\n",
        "\n",
        "        while True:\n",
        "            hunter_pos, prey_pos = random.sample(self.accessible_tiles, 2)\n",
        "            if self.check_accessibility(wall_map, hunter_pos, prey_pos):\n",
        "                break\n",
        "\n",
        "        self.hunter = Player(hunter_pos[0], hunter_pos[1], fov_radius=5, grid_size=grid_size)\n",
        "        self.prey   = Player(prey_pos[0],   prey_pos[1],   fov_radius=5, grid_size=grid_size)\n",
        "\n",
        "        self.hunter.update_vision(self.walls)\n",
        "        self.prey.update_vision(self.walls)\n",
        "\n",
        "    def generate_field(self, size):\n",
        "        p_set = 0.8\n",
        "        field = np.random.choice([0, 1], size=(size, size), p=[p_set, 1 - p_set])\n",
        "        field[0, :] = 1\n",
        "        field[-1, :] = 1\n",
        "        field[:, 0]  = 1\n",
        "        field[:, -1] = 1\n",
        "\n",
        "        wall_map = np.full((size, size), \".\", dtype=str)\n",
        "        wall_map[field == 1] = \"w\"\n",
        "\n",
        "        self.accessible_tiles = [(x, y) for x in range(size) for y in range(size) if wall_map[x][y] == \".\"]\n",
        "\n",
        "        return wall_map.tolist()\n",
        "\n",
        "    def check_accessibility(self, field, start, end):\n",
        "        queue = [start]\n",
        "        visited = set()\n",
        "        while queue:\n",
        "            x, y = queue.pop(0)\n",
        "            if (x, y) == end:\n",
        "                return True\n",
        "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "                nx, ny = x + dx, y + dy\n",
        "                if 0 <= nx < len(field) and 0 <= ny < len(field[0]) and (nx, ny) not in visited and field[nx][ny] == \".\":\n",
        "                    queue.append((nx, ny))\n",
        "                    visited.add((nx, ny))\n",
        "        return False\n",
        "\n",
        "    def get_state(self):\n",
        "        hunter_x, hunter_y = self.hunter.position\n",
        "        prey_x, prey_y = self.prey.position\n",
        "        hunter_sees_prey = 1 if self.hunter.can_see(self.prey.position) else 0\n",
        "        prey_sees_hunter = 1 if self.prey.can_see(self.hunter.position) else 0\n",
        "        dx = hunter_x - prey_x\n",
        "        dy = hunter_y - prey_y\n",
        "        return np.array([hunter_x, hunter_y, prey_x, prey_y, hunter_sees_prey, prey_sees_hunter, dx, dy], dtype=np.float32)\n",
        "\n",
        "    def step(self, hunter_action, prey_action):\n",
        "        self.hunter.move(hunter_action, self.walls)\n",
        "        self.prey.move(prey_action, self.walls)\n",
        "        dist = a_star_distance(self.walls, tuple(self.hunter.position),\n",
        "                               tuple(self.prey.position), self.grid_size)\n",
        "\n",
        "        if self.hunter.position == self.prey.position:\n",
        "            reward_hunter = +30\n",
        "            reward_prey   = -20.0\n",
        "            done = True\n",
        "        if dist is not None:\n",
        "            reward_hunter = -0.1*dist\n",
        "            reward_prey   = +0.1*dist\n",
        "            done = False\n",
        "        else:\n",
        "            reward_hunter, reward_prey = 0\n",
        "            done = True\n",
        "\n",
        "        return reward_hunter, reward_prey, done\n",
        "\n",
        "    def render(self, return_frame=False):\n",
        "        grid = [row[:] for row in self.walls]\n",
        "        hx, hy = self.hunter.position\n",
        "        px, py = self.prey.position\n",
        "        grid[hx][hy] = \"H\"\n",
        "        grid[px][py] = \"P\"\n",
        "        if return_frame:\n",
        "            return np.array(grid)\n",
        "        else:\n",
        "            os.system(\"cls\" if os.name == \"nt\" else \"clear\")\n",
        "            print(\"\\n\".join(\" \".join(row) for row in grid))\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "\n",
        "\n",
        "def a_star_distance(walls, start, goal, grid_size):\n",
        "    # Returns the length of the shortest path from 'start' to 'goal' in terms of number of steps\n",
        "    # Or None if there is no path\n",
        "    # If start is the target = 0\n",
        "    if start == goal:\n",
        "        return 0\n",
        "\n",
        "    (sx, sy) = start\n",
        "    (gx, gy) = goal\n",
        "\n",
        "    if walls[sx][sy] == \"w\" or walls[gx][gy] == \"w\":\n",
        "        return None\n",
        "    open_set = []\n",
        "    heapq.heappush(open_set, (0, sx, sy))\n",
        "    came_from = {}\n",
        "    cost_so_far = {(sx, sy): 0}\n",
        "\n",
        "    def heuristic(ax, ay, bx, by):\n",
        "        return abs(ax - bx) + abs(ay - by)\n",
        "\n",
        "    while open_set:\n",
        "        priority, cx, cy = heapq.heappop(open_set)\n",
        "\n",
        "        if (cx, cy) == (gx, gy):\n",
        "            return cost_so_far[(cx, cy)]\n",
        "\n",
        "        for dx, dy in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
        "            nx, ny = cx + dx, cy + dy\n",
        "            if 0 <= nx < grid_size and 0 <= ny < grid_size:\n",
        "                if walls[nx][ny] == \".\":\n",
        "                    new_cost = cost_so_far[(cx, cy)] + 1\n",
        "                    if (nx, ny) not in cost_so_far or new_cost < cost_so_far[(nx, ny)]:\n",
        "                        cost_so_far[(nx, ny)] = new_cost\n",
        "                        priority = new_cost + heuristic(nx, ny, gx, gy)\n",
        "                        heapq.heappush(open_set, (priority, nx, ny))\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "def save_animation(frames, filename, fps=12):\n",
        "    def frame_to_numeric(frame):\n",
        "        mapping = {\n",
        "            \".\": 8,   # Empty space\n",
        "            \"w\": 0,   # Wall\n",
        "            \"H\": 1,   # Hunter\n",
        "            \"P\": 2    # Prey\n",
        "        }\n",
        "        return np.vectorize(mapping.get)(frame)\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Convert each frame into a grid animation can process\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        ax.axis(\"off\")\n",
        "        numeric_frame = frame_to_numeric(frame)\n",
        "        ax.imshow(numeric_frame, cmap=\"gray\", vmin=0, vmax=3)\n",
        "\n",
        "    if frames:\n",
        "        frames.extend([frames[-1]] * int(3 * fps))\n",
        "\n",
        "    ani = anime.FuncAnimation(fig, update, frames=frames, interval=1000 / fps)\n",
        "    writer = PillowWriter(fps=fps)\n",
        "    ani.save(filename, writer = writer)\n",
        "    print(f\"Animation saved as {filename}\")\n",
        "\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "def train_hunter(hunter_agent, prey_agent, episodes, grid_size, turns, batch_size, render_on, n_try):\n",
        "    rewards_hunter = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        env = Environment(grid_size, turns)\n",
        "        state = env.get_state()\n",
        "        done = False\n",
        "        total_reward_hunter = 0.0\n",
        "        frames = []\n",
        "\n",
        "        for turn in range(turns):\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            hunter_action = hunter_agent.predict(state)\n",
        "            if len(prey_agent.replay_buffer) < 500:\n",
        "                prey_action = random.randint(0, 4)\n",
        "            else:\n",
        "                prey_action = prey_agent.predict(state)\n",
        "\n",
        "            next_state, reward_hunter, _, done = env.step(hunter_action, prey_action)\n",
        "\n",
        "            # Render the step\n",
        "            if render_on and episode >= episodes - 5:\n",
        "                    frames.append(env.render(return_frame=True))\n",
        "\n",
        "            hunter_agent.replay_buffer.push(\n",
        "                state, hunter_action, reward_hunter, next_state, done\n",
        "            )\n",
        "\n",
        "            hunter_agent.train(batch_size)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward_hunter += reward_hunter\n",
        "\n",
        "        hunter_agent.epsilon = max(hunter_agent.epsilon_min, hunter_agent.epsilon * hunter_agent.epsilon_decay)\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            hunter_agent.update_target_model()\n",
        "\n",
        "        rewards_hunter.append(total_reward_hunter)\n",
        "\n",
        "        print(f\"Episode {episode+1} out of {episodes}\")\n",
        "\n",
        "        if render_on and episode >= episodes - 5:\n",
        "            save_animation(frames, f\"hunter_episode_{n_try+1}_{episode+1}.gif\")\n",
        "\n",
        "    return rewards_hunter\n",
        "\n",
        "\n",
        "\n",
        "def train_prey(prey_agent, hunter_agent, episodes, grid_size, turns, batch_size, render_on, n_try):\n",
        "    rewards_prey = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        env = Environment(grid_size, turns)\n",
        "        state = env.get_state()\n",
        "        done = False\n",
        "        total_reward_prey = 0.0\n",
        "        frames = []\n",
        "\n",
        "        for turn in range(turns):\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            hunter_action = hunter_agent.predict(state)\n",
        "            prey_action   = prey_agent.predict(state)\n",
        "\n",
        "            next_state, _, reward_prey, done = env.step(hunter_action, prey_action)\n",
        "            prey_agent.replay_buffer.push(\n",
        "                state, prey_action, reward_prey, next_state, done\n",
        "            )\n",
        "\n",
        "            # Render the step\n",
        "            if render_on and episode >= episodes - 5:\n",
        "                    frames.append(env.render(return_frame=True))\n",
        "\n",
        "            prey_agent.train(batch_size)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward_prey += reward_prey\n",
        "\n",
        "        prey_agent.epsilon = max(prey_agent.epsilon_min, prey_agent.epsilon * prey_agent.epsilon_decay)\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            prey_agent.update_target_model()\n",
        "\n",
        "        rewards_prey.append(total_reward_prey)\n",
        "        print(f\"Episode {episode+1} out of {episodes}\")\n",
        "\n",
        "        if render_on and episode >= episodes - 5:\n",
        "            save_animation(frames, f\"prey_episode_{n_try+1}_{episode+1}.gif\")\n",
        "\n",
        "    return rewards_prey\n",
        "\n",
        "\n",
        "\n",
        "def train_IQL(hunter_agent, prey_agent, episodes_hunter, episodes_prey, grid_size, turns, batch_size, tries, render_on):\n",
        "    total_reward_hunter = []\n",
        "    total_reward_prey   = []\n",
        "    for n_try in range(tries):\n",
        "        print(f\"=== Switching sides! Hunter's turn {n_try+1} ===\")\n",
        "        rewards_hunter = train_hunter(hunter_agent, prey_agent, episodes_hunter, grid_size, turns, batch_size, render_on, n_try)\n",
        "        total_reward_hunter.extend(rewards_hunter)\n",
        "\n",
        "        print(f\"=== Switching sides! Prey's turn {n_try+1}! ===\")\n",
        "        rewards_prey = train_prey(prey_agent, hunter_agent, episodes_prey, grid_size, turns, batch_size, render_on, n_try)\n",
        "        total_reward_prey.extend(rewards_prey)\n",
        "\n",
        "    plt.close('all')\n",
        "    # MatPlotLib graphic output of the training cycle conducted\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    # Plot Hunter rewards\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(range(len(total_reward_hunter)), total_reward_hunter, label=\"Hunter\", color='#0E0598', s=5)\n",
        "    avg_hunter = [np.mean(total_reward_hunter[max(0, i-50):i+1]) for i in range(len(total_reward_hunter))]\n",
        "    plt.plot(range(len(total_reward_hunter)), avg_hunter, color='orange', label=\"Hunter Avg (50)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.title(f\"IQL Hunter:\\nGrid {grid_size}x{grid_size}; Turns {turns}; Episodes {episodes_hunter*tries*2}; FOV 5\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    # Plot Prey rewards\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(range(len(total_reward_prey)), total_reward_prey, label=\"Prey\", color='xkcd:baby poop green', s=5)\n",
        "    rolling_avg_prey = [np.mean(total_reward_prey[max(0, i-50):i+1]) for i in range(len(total_reward_prey))]\n",
        "    plt.plot(range(len(total_reward_prey)), rolling_avg_prey, color='green', label=\"Prey Avg (50)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.title(f\"IQL Prey:\\nGrid {grid_size}x{grid_size}; Turns {turns}; Episodes {episodes_hunter*tries*2}; FOV 5\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    grid_size = 12\n",
        "    hunter_agent = Agent(input_dim=8, n_actions=5)\n",
        "    prey_agent   = Agent(input_dim=8, n_actions=5)\n",
        "\n",
        "    # === SETTINGS === #\n",
        "    #\n",
        "    # episodes_hunter, episodes_prey -- Setup Hunter and Prey episodes for their unique\n",
        "    #                                   trainings (SHOULD BE EQUAL)\n",
        "    # grid_size                      -- set up the grid size of the map.\n",
        "    #                                   Do not compensate for walls (+2), they are not accounted for in the logic\n",
        "    # turns                          -- accounted for automatically\n",
        "    # batch size                     -- setup the batch size\n",
        "    # tries                          -- the number of cycles both of the models train\n",
        "    # render_on                      -- if you want to turn the render of the field on to see the process of training\n",
        "    #                                   May significantly impact the speed of the training\n",
        "    train_IQL(hunter_agent, prey_agent,\n",
        "              episodes_hunter=1000,\n",
        "              episodes_prey=1000,\n",
        "              grid_size=grid_size,\n",
        "              turns=int(grid_size*grid_size*2),\n",
        "              batch_size=32,\n",
        "              tries=3,\n",
        "              render_on = False)"
      ]
    }
  ]
}